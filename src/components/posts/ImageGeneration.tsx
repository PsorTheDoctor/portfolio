import styled from "styled-components";
import Kangaroo from "../../images/kangaroo.jpg";

type PropsType = {};

const Wrapper = styled.div`
  display: flex;
  flex-direction: column;
  max-width: 1000px;
  margin-left: auto;
  margin-right: auto;
  text-align: justify;
  margin-top: 10px;
  margin-bottom: 50px;
`;

const Title = styled.div`
  font-size: 32px;
  font-weight: bold;
`

const Image = styled.div`
  text-align: center;
  padding-top: 10px;
  padding-bottom: 30px;
`

export const ImageGeneration = (props: PropsType) => (
  <Wrapper>
    <Title>Testing image generation tools</Title>
    <p>Review of DALL-E text-to-image generator OpenAI kindly shared with me.</p>

    <p>DALL-E 2 is AI system that can create realistic images from description
    in natural language, image edits and variations. It's successor of DALL-E presented
    just a year before exceeding its possibilities. The best thing of this model is it can
    produce coherent illustrations by joining various concepts, attributes and styles of different level 
    of abstraction, such as kangaroo wearing an orange hoodie and blue sunglasses etc.</p>

    <Image>
      <img src={Kangaroo} width="600px" alt="Kangaroo wearing an orange hoodie and blue sunglasses" />
    </Image>

    <p><b>The rise of generative art</b></p>
    <p>In this place it should to be said that creating art by AI isn't a fresh idea
    and this concept refers to a field called generative art. Simply, it means art
    generated by machines. So far, it was dominated by Generative Adversarial Networks
    (StackGAN could even allow to very limited text-to-image generation). These days
    they're usually replaced by diffusion models. 2022 was especially prolific year 
    for them giving us DALL-E 2 followed by Google's Imagen and Parti.</p>

    <p>To avoid explicit usage of the model generation of some topics including 
    realistic human faces has been prohibited. Anyway, I was able to create the variations
    of mine fooling the censordhip system by FGSM (Fast Gradient Shift Method) attack.
    The results were rather quite satisfying.</p>

    <p>Telling the truth, it's actually third product OpenAI has shared with me. 
    Before that I was testing GPT-3 and its fine-tuned Codex model (GitHub Copilot 
    is powered by it). Being a witness of how they could write natural language and code
    also was amazing experience (GPT-3 even did my homework once), but images are a completely
    different story. It allows to see how much deep understanding actually is producing
    truly creative and unusual samples.
    </p>
  </Wrapper>
);